{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohithmsr/AI-practice/blob/main/Opik/0_intro_tracing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\" width=\"250\"/>"
      ],
      "metadata": {
        "id": "ALLVb0Gl10XR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L95CZYXbKMT"
      },
      "source": [
        "# Logging Traces with the Open AI Integration\n",
        "\n",
        "In this exercise, you'll log some basic traces with Opik. You can use OpenAI or open source models via LiteLLM. You can find [the full documentation for Opik's integration with OpenAI here](https://www.comet.com/docs/opik/tracing/integrations/openai). You can find [the full documentation for Opik's integration with LiteLLM here](https://www.comet.com/docs/opik/cookbook/litellm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZC2yFjVbKMV"
      },
      "source": [
        "# Imports & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "26FJE2wtbKMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c2993c-a4e5-41ae-c4db-b3a3c97d8965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.3/149.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.0/418.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install opik openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opik\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Define project name to enable tracing\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"Logging Traces Demo\""
      ],
      "metadata": {
        "id": "a-YtJBxp87ap"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"OPIK_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPIK_API_KEY\"] = getpass.getpass(\"Enter your Opik API key: \")\n",
        "\n",
        "opik.configure()"
      ],
      "metadata": {
        "id": "vLLeF-Fa9Mai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f183c6d7-dab5-413f-fd3d-08a94c019382"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Opik API key: ··········\n",
            "Do you want to use \"rohithmsr\" workspace? (Y/n)Y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Configuration saved to file: /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "QovBOUFNqZF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a92c8fe-f34c-48c4-fe75-67c230432891"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zWLzUfibKMW"
      },
      "source": [
        "# Tracking OpenAI Calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xbG-lxs-bKMW"
      },
      "outputs": [],
      "source": [
        "from opik.integrations.openai import track_openai\n",
        "from openai import OpenAI\n",
        "\n",
        "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "openai_client = track_openai(openai_client)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Hello, earth!\"\n",
        "\n",
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "      {\"role\":\"user\", \"content\":prompt}\n",
        "    ],\n",
        "    temperature=0.69,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "3hP2oybz9beN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9266567b-e1eb-4aa2-c8a3-ce897069ac7a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Open Source Models With LiteLLM\n",
        "Opik also integrates with LiteLLM, which allows you to use free open-source models and supports LLM APIs from all of the major providers (Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq, etc.) using the OpenAI format. [See here for a full list of LLM providers supported by LiteLLM as well as how to call them.](https://docs.litellm.ai/docs/providers)\n",
        "\n",
        "In the following example we'll use Meta's `Llama-3.1-8B-Instruct` model hosted on the Hugging Face hub.\n",
        "\n",
        "**If you have already run the OpenAI code above, you will need to restart your kernel before running the following code**"
      ],
      "metadata": {
        "id": "yg8N6_8Abd-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zvhvjuQBbKMW"
      },
      "outputs": [],
      "source": [
        "%pip install opik litellm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opik\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"Logging Traces LiteLLM\"\n",
        "\n",
        "if \"OPIK_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPIK_API_KEY\"] = getpass.getpass(\"Enter your Opik API key: \")\n",
        "\n",
        "opik.configure()"
      ],
      "metadata": {
        "id": "XMf1_TCHfMc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6299cdb6-408f-47fe-98a0-730ada296496"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Opik API key: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Opik is already configured. You can check the settings by viewing the config file at /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iouHRlmJbKMX"
      },
      "outputs": [],
      "source": [
        "from litellm.integrations.opik.opik import OpikLogger\n",
        "from opik.opik_context import get_current_span_data\n",
        "from opik import track\n",
        "import litellm\n",
        "\n",
        "opik_logger = OpikLogger()\n",
        "# In order to log LiteLLM traces to Opik, you will need to set the Opik callback\n",
        "litellm.callbacks = [opik_logger]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if \"HF_TOKEN\" not in os.environ:\n",
        "    os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face API key: \")"
      ],
      "metadata": {
        "id": "fdWUie6kroz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a808a2d-b3f6-4649-d60f-e46fd6349bc6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vL8Ze4ShbKMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccef0d19-d191-4587-afad-78f61e612d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, so there's a llama in my garden and I need to figure out what to do. First, I should stay calm. Panicking might scare the llama more. I remember that llamas are generally gentle but can be skittish. Maybe I should keep my distance to avoid stressing it. I should check if it's alone or part of a group. If it's part of a herd, there might be more llamas nearby.\n",
            "\n",
            "I need to make sure it's not injured. If it is, I should contact animal control or a vet. But if it's healthy, maybe it's just wandered off. I should look around for any identification tags or collars. If there's a phone number, I can call the owner directly. If not, I can check with local farms or llama owners in the area. \n",
            "\n",
            "I should also secure my garden to prevent the llama from causing damage. Maybe close gates or put up barriers. But I shouldn't try to corner it. Maybe offer some food or water to keep it calm while I contact someone. Wait, what kind of food is safe for llamas? I think they eat hay or grass, so maybe some hay would be better than other snacks. \n",
            "\n",
            "I should also inform the local authorities, like the police or animal control, so they can help find the owner. Maybe put up a sign in the neighborhood to alert others. I shouldn't try to chase it away because that might make it run into traffic or get hurt. \n",
            "\n",
            "If the llama stays for a while, I might need to provide temporary shelter, but I'm not sure how to do that. Maybe just keep an eye on it until help arrives. Also, take photos in case the owner needs proof of the llama's condition. \n",
            "\n",
            "Hmm, I should also consider if there are any local regulations about stray animals. Maybe check online or call the town office. Oh, and make sure my pets are safe, keeping them indoors so they don't provoke the llama. \n",
            "\n",
            "Wait, what if the llama is dangerous? Though I think they're usually not aggressive unless threatened. Still, better to be cautious. Maybe keep children and other animals away. \n",
            "\n",
            "So the steps would be: stay calm, secure the area, check for ID, contact owner or authorities, provide food/water if possible, and wait for help. I should also document everything with photos and notes. Yeah, that seems like a plan.\n",
            "</think>\n",
            "\n",
            "If you find a llama in your garden, here's a step-by-step guide to handle the situation safely and responsibly:\n",
            "\n",
            "### 1. **Stay Calm and Observe**\n",
            "   - **Keep your distance**: Avoid sudden movements or loud noises, as llamas can be skittish. Maintain a safe distance to prevent startling it.\n",
            "   - **Assess its behavior**: Is it calm, injured, or agitated? Note any visible injuries or unusual behavior.\n",
            "\n",
            "### 2. **Check for Identification**\n",
            "   - Look for collars, tags, or brands that might indicate ownership. If found, contact the listed number immediately.\n",
            "\n",
            "### 3. **Secure the Area**\n",
            "   - **Close gates/fences**: Prevent the llama from wandering further or causing damage. Ensure your pets are indoors to avoid provoking it.\n",
            "   - **Avoid cornering it**: Llamas may panic if trapped. Allow them space to move freely.\n",
            "\n",
            "### 4. **Provide Basic Care (If Safe)**\n",
            "   - **Offer food/water**: Llamas eat grass, hay, or leafy greens. Place these nearby but don’t approach directly.\n",
            "   - **Avoid unfamiliar foods**: Never give processed snacks, as they can be harmful.\n",
            "\n",
            "### 5. **Contact Authorities and Potential Owners**\n",
            "   - **Local animal control or police**: Report the stray llama. They can assist in locating the owner or ensuring its safety.\n",
            "   - **Reach out to farms or llama owners**: Use online directories or social media to alert local farms or breeders.\n",
            "   - **Post on community boards**: Share photos and details on neighborhood groups (e.g., Nextdoor, Facebook) to find the owner quickly.\n",
            "\n",
            "### 6. **Document the Situation**\n",
            "   - Take clear photos of the llama (including any ID) and note its location, behavior, and time of discovery. This helps in reuniting it with its owner.\n",
            "\n",
            "### 7. **Wait for Assistance**\n",
            "   - Stay nearby but at a distance until help arrives. Do not attempt to capture or transport the llama yourself, as this can be dangerous.\n",
            "\n",
            "### 8. **Consider Local Regulations**\n",
            "   - Check with your town/city office about stray animal protocols. Some areas require reporting to specific agencies.\n",
            "\n",
            "### 9. **Keep Others Informed**\n",
            "   - Alert neighbors to keep their pets indoors and avoid approaching the llama. Post a temporary sign in your area to spread awareness.\n",
            "\n",
            "### 10. **Follow Up**\n",
            "   - Once the llama is collected, ensure the owner or authorities confirm its safe return. If unclaimed, inquire about local shelters or sanctuaries that accept llamas.\n",
            "\n",
            "### Key Reminders:\n",
            "- **Llamas are generally gentle**, but they can kick or spit if threatened. Always prioritize safety.\n",
            "- **Never chase the animal**, as this may lead to injury or traffic accidents.\n",
            "- **Document everything** to assist in the return process.\n",
            "\n",
            "By following these steps, you’ll help ensure the llama’s well-being and safely resolve the situation. 🦙✨\n"
          ]
        }
      ],
      "source": [
        "messages = [{ \"content\": \"There's a llama in my garden 😱 What should I do?\",\"role\": \"user\"}]\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"huggingface/Qwen/QwQ-32B\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dnp1IyATVTUk"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "comet-eval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rZC2yFjVbKMV",
        "5zWLzUfibKMW",
        "yg8N6_8Abd-g"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}